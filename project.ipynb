{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk                            import word_tokenize\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tokenize                   import TweetTokenizer\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from wordcloud                       import WordCloud\n",
    "from collections                     import Counter\n",
    "from gensim.models                   import Word2Vec\n",
    "from sklearn.manifold                import TSNE\n",
    "from IPython.core.display            import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "dl = ndf.values.tolist()\n",
    "\n",
    "# dl = df.values.tolist()\n",
    "\n",
    "tknzr       = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "\n",
    "for item in dl:\n",
    "    \n",
    "    tweet = item[3]\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*' , '' , tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    print(temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"we're\" , \"you're\" , \"they're\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "    \n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "    \n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp);\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "for i,tok in enumerate(tokens):\n",
    "    print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(fusedTokens)\n",
    "print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(positives)\n",
    "print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(negatives)\n",
    "print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(neutrals)\n",
    "print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color=\"white\" , width=640 , height=320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color=\"white\" , width=640 , height=320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color=\"white\" , width=640 , height=320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color=\"white\" , width=640 , height=320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=100) \n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets âˆ™ vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_vectorizer.get_feature_names())\n",
    "\n",
    "for box in bow_xtrain.toarray():\n",
    "    print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100) \n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print( tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"tfidf.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokens,\n",
    "            size=10, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples= len(tokens), epochs=20)\n",
    "\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"wordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_w2v.wv.most_similar(positive=\"college\")\n",
    "model_w2v.wv.__getitem__(\"college\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
