{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk                            import word_tokenize\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tokenize                   import TweetTokenizer\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from wordcloud                       import WordCloud\n",
    "from collections                     import Counter\n",
    "from gensim.models                   import Word2Vec\n",
    "from sklearn.manifold                import TSNE\n",
    "from IPython.core.display            import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ITEM =  [264183816548130816, '15140428', 'positive', \"Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\"]\n",
      "TEMP 1 =  ['gas', 'by', 'my', 'house', 'hit', '.', '39', '!', '!', '!', \"i'm\", 'going', 'to', 'chapel', 'hill', 'on', 'sat', '.', ':)']\n",
      "TEMP 2 =  ['gas', 'house', 'hit', '.', '39', '!', '!', '!', 'going', 'chapel', 'hill', 'sat', '.', ':)']\n",
      "TEMP 3 =  ['gas', 'house', 'hit', '', '39', '', '', '', 'going', 'chapel', 'hill', 'sat', '', '']\n",
      "TEMP 4 =  ['gas', 'house', 'hit', 'going', 'chapel', 'hill', 'sat']\n",
      "\n",
      "ITEM =  [263405084770172928, '591166521', 'negative', 'Theo Walcott is still shit, watch Rafa and Johnny deal with him on Saturday.']\n",
      "TEMP 1 =  ['theo', 'walcott', 'is', 'still', 'shit', ',', 'watch', 'rafa', 'and', 'johnny', 'deal', 'with', 'him', 'on', 'saturday', '.']\n",
      "TEMP 2 =  ['theo', 'walcott', 'still', 'shit', ',', 'watch', 'rafa', 'johnny', 'deal', 'saturday', '.']\n",
      "TEMP 3 =  ['theo', 'walcott', 'still', 'shit', '', 'watch', 'rafa', 'johnny', 'deal', 'saturday', '']\n",
      "TEMP 4 =  ['theo', 'walcott', 'still', 'shit', 'watch', 'rafa', 'johnny', 'deal', 'saturday']\n",
      "\n",
      "ITEM =  [262163168678248449, '35266263', 'negative', \"its not that I'm a GSP fan, i just hate Nick Diaz. can't wait for february.\"]\n",
      "TEMP 1 =  ['its', 'not', 'that', \"i'm\", 'a', 'gsp', 'fan', ',', 'i', 'just', 'hate', 'nick', 'diaz', '.', \"can't\", 'wait', 'for', 'february', '.']\n",
      "TEMP 2 =  ['gsp', 'fan', ',', 'hate', 'nick', 'diaz', '.', \"can't\", 'wait', 'february', '.']\n",
      "TEMP 3 =  ['gsp', 'fan', '', 'hate', 'nick', 'diaz', '', 'cant', 'wait', 'february', '']\n",
      "TEMP 4 =  ['gsp', 'fan', 'hate', 'nick', 'diaz', 'cant', 'wait', 'february']\n",
      "\n",
      "ITEM =  [264249301910310912, '18516728', 'negative', \"Iranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)\"]\n",
      "TEMP 1 =  ['iranian', 'general', 'says', \"israel's\", 'iron', 'dome', \"can't\", 'deal', 'with', 'their', 'missiles', '(', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out', ')']\n",
      "TEMP 2 =  ['iranian', 'general', 'says', \"israel's\", 'iron', 'dome', \"can't\", 'deal', 'missiles', '(', 'keep', 'talking', 'like', 'may', 'end', 'finding', ')']\n",
      "TEMP 3 =  ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'missiles', '', 'keep', 'talking', 'like', 'may', 'end', 'finding', '']\n",
      "TEMP 4 =  ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'missiles', 'keep', 'talking', 'like', 'may', 'end', 'finding']\n",
      "\n",
      "ITEM =  [262682041215234048, '254373818', 'neutral', 'Tehran, Mon Amour: Obama Tried to Establish Ties with the Mullahs http://t.co/TZZzrrKa via @PJMedia_com No Barack Obama - Vote Mitt Romney']\n",
      "TEMP 1 =  ['tehran', ',', 'mon', 'amour', ':', 'obama', 'tried', 'to', 'establish', 'ties', 'with', 'the', 'mullahs']\n",
      "TEMP 2 =  ['tehran', ',', 'mon', 'amour', ':', 'obama', 'tried', 'establish', 'ties', 'mullahs']\n",
      "TEMP 3 =  ['tehran', '', 'mon', 'amour', '', 'obama', 'tried', 'establish', 'ties', 'mullahs']\n",
      "TEMP 4 =  ['tehran', 'mon', 'amour', 'obama', 'tried', 'establish', 'ties', 'mullahs']\n",
      "\n",
      "ITEM =  [264229576773861376, '518129399', 'neutral', 'I sat through this whole movie just for Harry and Ron at christmas. ohlawd']\n",
      "TEMP 1 =  ['i', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'harry', 'and', 'ron', 'at', 'christmas', '.', 'ohlawd']\n",
      "TEMP 2 =  ['sat', 'whole', 'movie', 'harry', 'ron', 'christmas', '.', 'ohlawd']\n",
      "TEMP 3 =  ['sat', 'whole', 'movie', 'harry', 'ron', 'christmas', '', 'ohlawd']\n",
      "TEMP 4 =  ['sat', 'whole', 'movie', 'harry', 'ron', 'christmas', 'ohlawd']\n",
      "\n",
      "ITEM =  [264105751826538497, '147088367', 'positive', 'with J Davlar 11th. Main rivals are team Poland. Hopefully we an make it a successful end to a tough week of training tomorrow.']\n",
      "TEMP 1 =  ['with', 'j', 'davlar', '11th', '.', 'main', 'rivals', 'are', 'team', 'poland', '.', 'hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "TEMP 2 =  ['j', 'davlar', '11th', '.', 'main', 'rivals', 'team', 'poland', '.', 'hopefully', 'make', 'successful', 'end', 'tough', 'week', 'training', 'tomorrow', '.']\n",
      "TEMP 3 =  ['j', 'davlar', '11th', '', 'main', 'rivals', 'team', 'poland', '', 'hopefully', 'make', 'successful', 'end', 'tough', 'week', 'training', 'tomorrow', '']\n",
      "TEMP 4 =  ['j', 'davlar', 'main', 'rivals', 'team', 'poland', 'hopefully', 'make', 'successful', 'end', 'tough', 'week', 'training', 'tomorrow']\n",
      "\n",
      "ITEM =  [264094586689953794, '332474633', 'negative', \"Talking about ACT's && SAT's, deciding where I want to go to college, applying to colleges and everything about college stresses me out.\"]\n",
      "TEMP 1 =  ['talking', 'about', \"act's\", '&', '&', \"sat's\", ',', 'deciding', 'where', 'i', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.']\n",
      "TEMP 2 =  ['talking', \"act's\", '&', '&', \"sat's\", ',', 'deciding', 'want', 'go', 'college', ',', 'applying', 'colleges', 'everything', 'college', 'stresses', '.']\n",
      "TEMP 3 =  ['talking', 'acts', '', '', 'sats', '', 'deciding', 'want', 'go', 'college', '', 'applying', 'colleges', 'everything', 'college', 'stresses', '']\n",
      "TEMP 4 =  ['talking', 'acts', 'sats', 'deciding', 'want', 'go', 'college', 'applying', 'colleges', 'everything', 'college', 'stresses']\n",
      "\n",
      "ITEM =  [212392538055778304, '274996324', 'neutral', 'Why is \\\\\"\"Happy Valentines Day\\\\\"\" trending? It\\'s on the 14th of February not 12th of June smh..']\n",
      "TEMP 1 =  ['why', 'is', '\\\\', '\"', '\"', 'happy', 'valentines', 'day', '\\\\', '\"', '\"', 'trending', '?', \"it's\", 'on', 'the', '14th', 'of', 'february', 'not', '12th', 'of', 'june', 'smh', '..']\n",
      "TEMP 2 =  ['\\\\', '\"', '\"', 'happy', 'valentines', 'day', '\\\\', '\"', '\"', 'trending', '?', '14th', 'february', '12th', 'june', 'smh', '..']\n",
      "TEMP 3 =  ['', '', '', 'happy', 'valentines', 'day', '', '', '', 'trending', '', '14th', 'february', '12th', 'june', 'smh', '']\n",
      "TEMP 4 =  ['happy', 'valentines', 'day', 'trending', 'february', 'june', 'smh']\n",
      "\n",
      "ITEM =  [254941790757601280, '557103111', 'negative', \"They may have a SuperBowl in Dallas, but Dallas ain't winning a SuperBowl. Not with that quarterback and owner. @S4NYC @RasmussenPoll\"]\n",
      "TEMP 1 =  ['they', 'may', 'have', 'a', 'superbowl', 'in', 'dallas', ',', 'but', 'dallas', \"ain't\", 'winning', 'a', 'superbowl', '.', 'not', 'with', 'that', 'quarterback', 'and', 'owner', '.']\n",
      "TEMP 2 =  ['may', 'superbowl', 'dallas', ',', 'dallas', \"ain't\", 'winning', 'superbowl', '.', 'quarterback', 'owner', '.']\n",
      "TEMP 3 =  ['may', 'superbowl', 'dallas', '', 'dallas', 'aint', 'winning', 'superbowl', '', 'quarterback', 'owner', '']\n",
      "TEMP 4 =  ['may', 'superbowl', 'dallas', 'dallas', 'aint', 'winning', 'superbowl', 'quarterback', 'owner']\n",
      "\n",
      "\u001b[1;33mPrinting Tokens\u001b[0m\n",
      "\u001b[1;34m->\u001b[0m ['gas', 'house', 'hit', 'going', 'chapel', 'hill', 'sat']\n",
      "\u001b[1;34m->\u001b[0m ['theo', 'walcott', 'still', 'shit', 'watch', 'rafa', 'johnny', 'deal', 'saturday']\n",
      "\u001b[1;34m->\u001b[0m ['gsp', 'fan', 'hate', 'nick', 'diaz', 'cant', 'wait', 'february']\n",
      "\u001b[1;34m->\u001b[0m ['iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'missiles', 'keep', 'talking', 'like', 'may', 'end', 'finding']\n",
      "\u001b[1;34m->\u001b[0m ['tehran', 'mon', 'amour', 'obama', 'tried', 'establish', 'ties', 'mullahs']\n",
      "\u001b[1;34m->\u001b[0m ['sat', 'whole', 'movie', 'harry', 'ron', 'christmas', 'ohlawd']\n",
      "\u001b[1;34m->\u001b[0m ['j', 'davlar', 'main', 'rivals', 'team', 'poland', 'hopefully', 'make', 'successful', 'end', 'tough', 'week', 'training', 'tomorrow']\n",
      "\u001b[1;34m->\u001b[0m ['talking', 'acts', 'sats', 'deciding', 'want', 'go', 'college', 'applying', 'colleges', 'everything', 'college', 'stresses']\n",
      "\u001b[1;34m->\u001b[0m ['happy', 'valentines', 'day', 'trending', 'february', 'june', 'smh']\n",
      "\u001b[1;34m->\u001b[0m ['may', 'superbowl', 'dallas', 'dallas', 'aint', 'winning', 'superbowl', 'quarterback', 'owner']\n",
      "\n",
      "\u001b[1;33mPrinting Fuzed Tokens\u001b[0m\n",
      "['gas', 'house', 'hit', 'going', 'chapel', 'hill', 'sat', 'theo', 'walcott', 'still', 'shit', 'watch', 'rafa', 'johnny', 'deal', 'saturday', 'gsp', 'fan', 'hate', 'nick', 'diaz', 'cant', 'wait', 'february', 'iranian', 'general', 'says', 'israels', 'iron', 'dome', 'cant', 'deal', 'missiles', 'keep', 'talking', 'like', 'may', 'end', 'finding', 'tehran', 'mon', 'amour', 'obama', 'tried', 'establish', 'ties', 'mullahs', 'sat', 'whole', 'movie', 'harry', 'ron', 'christmas', 'ohlawd', 'j', 'davlar', 'main', 'rivals', 'team', 'poland', 'hopefully', 'make', 'successful', 'end', 'tough', 'week', 'training', 'tomorrow', 'talking', 'acts', 'sats', 'deciding', 'want', 'go', 'college', 'applying', 'colleges', 'everything', 'college', 'stresses', 'happy', 'valentines', 'day', 'trending', 'february', 'june', 'smh', 'may', 'superbowl', 'dallas', 'dallas', 'aint', 'winning', 'superbowl', 'quarterback', 'owner']\n"
     ]
    }
   ],
   "source": [
    "ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "dl = ndf.values.tolist()\n",
    "\n",
    "# dl = df.values.tolist()\n",
    "\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "    print(\"\\nITEM = \",item)\n",
    "    tweet = item[3]\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*' , '' , tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    print(\"TEMP 1 = \",temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "    print(\"TEMP 2 = \",temp)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "    print(\"TEMP 3 = \",temp)\n",
    "\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "    print(\"TEMP 4 = \",temp)\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp);\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "for i,tok in enumerate(tokens):\n",
    "    print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(fusedTokens)\n",
    "print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(positives)\n",
    "print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(negatives)\n",
    "print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(neutrals)\n",
    "print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"white\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"red\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"green\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"blue\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=100) \n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets ∙ vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bow_vectorizer.get_feature_names())\n",
    "\n",
    "# for box in bow_xtrain.toarray():\n",
    "#     print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100) \n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"tfidf.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "featuresSize = 10\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = 10, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"wordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_w2v.wv.most_similar(positive=\"college\")\n",
    "model_w2v.wv.__getitem__(\"college\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize = (16,16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy = (x[i], y[i]), xytext = (5,2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum  = 1\n",
    "allDicts = []\n",
    "\n",
    "dictLocation = \"./lexica/generic/generic.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "genericDict = []\n",
    "for line in file:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(float(word))\n",
    "        count += 1\n",
    "    genericDict.extend([temp])\n",
    "    \n",
    "allDicts.extend([genericDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictNum += 1\n",
    "\n",
    "# dictLocation = \"./lexica/emotweet/valence_tweet.txt\"\n",
    "# file = open(dictLocation, \"r\")\n",
    "# dic = []\n",
    "# for line in file:\n",
    "#     temp = []\n",
    "#     count = 1\n",
    "#     for word in line.split():\n",
    "#         if count == 1:\n",
    "#             temp.append(word)\n",
    "#         else:\n",
    "#             temp.append(float(word))\n",
    "#         count += 1\n",
    "#     dic.extend([temp])\n",
    "    \n",
    "# allDicts.extend([dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "\n",
    "for sentence in tokens:\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "    \n",
    "    for dic in allDicts:\n",
    "        wordCount = 0\n",
    "        value     = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            for token in dic:\n",
    "                if word == token[0]:\n",
    "                    wordCount += 1\n",
    "                    value     += token[1]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "        print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "print(allTweetFeatsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
