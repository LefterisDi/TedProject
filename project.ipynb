{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import gensim\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk                            import word_tokenize\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.tokenize                   import TweetTokenizer\n",
    "from nltk.stem.porter                import PorterStemmer\n",
    "from wordcloud                       import WordCloud\n",
    "from collections                     import Counter\n",
    "from gensim.models                   import Word2Vec\n",
    "from sklearn.manifold                import TSNE\n",
    "from IPython.core.display            import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn                         import svm\n",
    "from sklearn.metrics                 import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(location , sep = \"\\t\" , header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = df.head(10) #takes the first x entries\n",
    "\n",
    "dl = ndf.values.tolist()\n",
    "\n",
    "# dl = df.values.tolist()\n",
    "\n",
    "emotions    = []\n",
    "positives   = []\n",
    "negatives   = []\n",
    "neutrals    = []\n",
    "tokens      = []\n",
    "fusedTokens = []\n",
    "tknzr       = TweetTokenizer(preserve_case = False, \n",
    "                             strip_handles = True, \n",
    "                             reduce_len    = True)\n",
    "for item in dl:\n",
    "    print(\"\\nITEM = \",item)\n",
    "    tweet = item[3]\n",
    "    emotions.append(item[2])\n",
    "#     if item[2] == \"positive\":\n",
    "#         emotions.append(1)\n",
    "#     elif item[2] == \"negative\":\n",
    "#         emotions.append(-1)\n",
    "#     elif item[2] == \"neutral\":\n",
    "#         emotions.append(0)\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*' , '' , tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+' , '' , tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "#     tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove hashtags, removing the hash (#) sign only from the word\n",
    "    tweet = re.sub(r'#' , '' , tweet)\n",
    "    \n",
    "    temp = tknzr.tokenize(tweet)\n",
    "    \n",
    "    print(\"TEMP 1 = \",temp)\n",
    "    \n",
    "#     temp = [w.lower() for w in temp] #convert to lower case\n",
    "    \n",
    "    stop_words = stopwords.words('english')    #sets stop words\n",
    "    newStopWords = [\"i'm\" , \"he's\" , \"she's\" , \"it's\" , \"we're\" , \"you're\" , \"they're\" , \"via\"]\n",
    "    stop_words.extend(newStopWords)\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    temp = [w for w in temp if not w in stop_words]  #removes stop words\n",
    "    print(\"TEMP 2 = \",temp)\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation) #remove punctuation\n",
    "    temp = [w.translate(table) for w in temp]\n",
    "    print(\"TEMP 3 = \",temp)\n",
    "\n",
    "    temp = [word for word in temp if word.isalpha()] #remove remaining tokens that are not alphabetic\n",
    "    print(\"TEMP 4 = \",temp)\n",
    "\n",
    "#     porter = PorterStemmer() #stemming (not that useful)\n",
    "#     temp = [porter.stem(word) for word in temp]\n",
    "\n",
    "    if item[2] == \"positive\":  #need to give the words positive and negative weight so that the most common words in positive posts is not \"tomorrow\"\n",
    "        positives.extend(temp)\n",
    "    elif item[2] == \"negative\":\n",
    "        negatives.extend(temp)\n",
    "    elif item[2] == \"neutral\":\n",
    "        neutrals.extend(temp)\n",
    "        \n",
    "    fusedTokens.extend(temp)\n",
    "    tokens.append(temp)\n",
    "    \n",
    "print(\"\\n\\033[1;33mPrinting Tokens\\033[0m\")\n",
    "for i,tok in enumerate(tokens):\n",
    "    print(\"\\033[1;34m->\\033[0m\",tok)\n",
    "print(\"\\n\\033[1;33mPrinting Fuzed Tokens\\033[0m\")\n",
    "print(fusedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(fusedTokens)\n",
    "print(\"\\033[1;33mGenerally most common words:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(positives)\n",
    "print(\"\\033[1;33m\\nMost common words found in positive posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(negatives)\n",
    "print(\"\\033[1;33m\\nMost common words found in negative posts:\\033[0m\\n\" , count.most_common(10))\n",
    "\n",
    "count = Counter(neutrals)\n",
    "print(\"\\033[1;33m\\nMost common words found in neutral posts:\\033[0m\\n\" , count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"\"\n",
    "for word in fusedTokens:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"white\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in positives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"red\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in negatives:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"green\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------------#\n",
    "\n",
    "final = \"\"\n",
    "for word in neutrals:\n",
    "    final += \" \" + word\n",
    "    \n",
    "wc = WordCloud(background_color = \"blue\", width = 640, height = 320).generate(final)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that words like \"love\" appear in positive posts as expected , whereas \"positive\" words like \"like\" appear in negative posts . Also there are many neutral words like \"tomorrow\" that have the same distribution in both positive and negative posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTokens = []\n",
    "for item in tokens:\n",
    "    final = \"\"\n",
    "    for word in item:\n",
    "        final += \" \" + word\n",
    "    print(final)\n",
    "    newTokens.append(final)\n",
    "    \n",
    "# bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=100, stop_words='english')\n",
    "bow_vectorizer = CountVectorizer(max_features=100) \n",
    "bow_xtrain = bow_vectorizer.fit_transform(newTokens)  #TWEETS : a list with the actual tweets \n",
    "# print(bow_vectorizer.get_feature_names())\n",
    "# print(bow_xtrain.toarray())\n",
    "\n",
    "# The output is a numpy array of features. The dimensionality of this array \n",
    "# depends on the number of TWEETS (shape should be number_of_tweets âˆ™ vocabulary_size) \n",
    "print(bow_xtrain.shape)\n",
    "\n",
    "# filename = \"bow.pkl\"\n",
    "outfile = open(\"bow.pkl\" , \"wb\")\n",
    "pickle.dump(bow_xtrain , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bow_vectorizer.get_feature_names())\n",
    "\n",
    "# for box in bow_xtrain.toarray():\n",
    "#     print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100) \n",
    "tfidf = tfidf_vectorizer.fit_transform(newTokens)\n",
    "print(tfidf.shape) #the output is a numpy array of features\n",
    "\n",
    "\n",
    "# filename = \"tfidf.pkl\"\n",
    "outfile = open(\"tfidf.pkl\" , \"wb\")\n",
    "pickle.dump(tfidf , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tweet = tweets.apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "featuresSize = 10\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(tokens,\n",
    "                                   size      = 10, # desired no. of features/independent variables\n",
    "                                   window    = 5,  # context window size\n",
    "                                   min_count = 2,\n",
    "                                   sg        = 1,  # 1 for skip-gram model\n",
    "                                   hs        = 0,\n",
    "                                   negative  = 10, # for negative sampling\n",
    "                                   workers   = 2,  # no.of cores\n",
    "                                   seed      = 34) \n",
    "\n",
    "model_w2v.train(tokens, total_examples = len(tokens), epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_w2v.wv.most_similar(positive=\"college\")\n",
    "model_w2v.wv.__getitem__(\"college\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model.wv.__getitem__(word))\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity = 40, n_components = 2, init = 'pca', n_iter = 2500, random_state = 23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize = (16,16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy = (x[i], y[i]), xytext = (5,2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictNum  = 1\n",
    "allDicts = []\n",
    "\n",
    "dictLocation = \"./lexica/generic/generic.txt\"\n",
    "file = open(dictLocation, \"r\")\n",
    "genericDict = []\n",
    "for line in file:\n",
    "    temp  = []\n",
    "    count = 1\n",
    "    for word in line.split():\n",
    "        if count == 1:\n",
    "            temp.append(word)\n",
    "        else:\n",
    "            temp.append(float(word))\n",
    "        count += 1\n",
    "    genericDict.extend([temp])\n",
    "    \n",
    "allDicts.extend([genericDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictNum += 1\n",
    "\n",
    "# dictLocation = \"./lexica/emotweet/valence_tweet.txt\"\n",
    "# file = open(dictLocation, \"r\")\n",
    "# dic = []\n",
    "# for line in file:\n",
    "#     temp = []\n",
    "#     count = 1\n",
    "#     for word in line.split():\n",
    "#         if count == 1:\n",
    "#             temp.append(word)\n",
    "#         else:\n",
    "#             temp.append(float(word))\n",
    "#         count += 1\n",
    "#     dic.extend([temp])\n",
    "    \n",
    "# allDicts.extend([dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model_w2v.wv\n",
    "allTweetFeatsList = []\n",
    "\n",
    "for sentence in tokens:\n",
    "    \n",
    "    tweetFeatures = []\n",
    "    \n",
    "    for i in range(0,featuresSize):\n",
    "        value     = 0\n",
    "        wordCount = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in word_vectors.vocab:\n",
    "                wordCount += 1\n",
    "                value     += word_vectors[word][i]\n",
    "                \n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "    \n",
    "    for dic in allDicts:\n",
    "        wordCount = 0\n",
    "        value     = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            for token in dic:\n",
    "                if word == token[0]:\n",
    "                    wordCount += 1\n",
    "                    value     += token[1]\n",
    "\n",
    "        if wordCount != 0:\n",
    "            tweetFeatures.append(value / wordCount)\n",
    "        else:\n",
    "            tweetFeatures.append(0)\n",
    "\n",
    "        print(tweetFeatures , \"\\n\")\n",
    "    \n",
    "            \n",
    "    allTweetFeatsList.extend([tweetFeatures])\n",
    "\n",
    "print(allTweetFeatsList)\n",
    "\n",
    "# filename = \"wordEmbs.pkl\"\n",
    "outfile = open(\"wordEmbs.pkl\" , \"wb\")\n",
    "pickle.dump(allTweetFeatsList , outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"bow.pkl\" , \"rb\")\n",
    "bow_xtrain = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "#input for this method is any array of features\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow_xtrain, emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "svc = svc.fit(xtrain_bow, ytrain) # xtrain_bow:bag of words features for train data, ytrain: train data labels\n",
    "\n",
    "probPrediction = svc.predict_proba(xvalid_bow) #predict on the validation set\n",
    "prediction_int = svc.predict(xvalid_bow)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "f1_score(yvalid, prediction_int , average=\"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"tfidf.pkl\" , \"rb\")\n",
    "tfidf  = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "#input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(tfidf , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "svc = svc.fit(xtrain, ytrain) # xtrain:tfidf features for train data, ytrain: train data labels\n",
    "\n",
    "probPrediction = svc.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svc.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "f1_score(yvalid, prediction_int , average = \"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"wordEmbs.pkl\" , \"rb\")\n",
    "allTweetFeatsList = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "#input for this method is any array of features\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(allTweetFeatsList , emotions, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel = 'linear', C = 1, probability = True)\n",
    "svc = svc.fit(xtrain, ytrain) # xtrain:word2vec features for train data, ytrain: train data labels\n",
    "\n",
    "probPrediction = svc.predict_proba(xvalid) #predict on the validation set\n",
    "prediction_int = svc.predict(xvalid)\n",
    "prediction_int = prediction_int.tolist()\n",
    "\n",
    "f1_score(yvalid, prediction_int , average = \"micro\") #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
